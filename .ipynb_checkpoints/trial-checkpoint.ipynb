{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27136ccd-ac13-4101-a6f4-47d20e435383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import Dataset, load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os \n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import joblib\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "violet = '#702b9d'\n",
    "RANDOM_SEED = 1002\n",
    "\n",
    "checkpoint = 'FacebookAI/xlm-roberta-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6890a86-35b0-4719-876e-43a962a1c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_train(example):\n",
    "    with open(os.getcwd() + '/data_en/en.intents', 'r', encoding = 'UTF-8') as file:\n",
    "        intent_labels_map = json.load(file)\n",
    "    \n",
    "    with open(os.getcwd() + '/data_en/en.slots', 'r', encoding = 'UTF-8') as file:\n",
    "        slot_labels_map = json.load(file)\n",
    "    \n",
    "        \n",
    "    label_to_pred_intent_idx = {v: k for k, v in intent_labels_map.items()}\n",
    "    \n",
    "    label_to_pred_slot_idx = {v: k for k, v in slot_labels_map.items()}\n",
    "    \n",
    "    converted_intent_labels = [int(label_to_pred_intent_idx.get(p, -100)) for p in example['target_intents']]  \n",
    "    converted_slot_labels = [[int(label_to_pred_slot_idx.get(s, -100)) for s in slot_seq] for slot_seq in example['target_slots']]\n",
    "    example['target_intents'], example['target_slots'] = converted_intent_labels, converted_slot_labels\n",
    "    return example\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db661412-97a7-44e8-ab3f-e6029b4dc7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train = Dataset.from_file(os.getcwd() + '/data_en/en.train/data-00000-of-00001.arrow')\n",
    "zh_train = Dataset.from_file(os.getcwd() + '/data_zh/zh.train/data-00000-of-00001.arrow')\n",
    "    \n",
    "zh_val = Dataset.from_file(os.getcwd() + '/data_zh/zh.dev/data-00000-of-00001.arrow')\n",
    "para_dataset = deepcopy(en_train)\n",
    "para_dataset = para_dataset.add_column(\"target_utt\", zh_train['utt'])\n",
    "para_dataset = para_dataset.add_column(\"target_slots\", zh_train['slots_str'])\n",
    "para_dataset = para_dataset.add_column(\"target_intents\", zh_train['intent_str'])\n",
    "para_dataset = para_dataset.map(lambda x: convert_train(x), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dec878e8-a8ad-4a4d-af7e-b744370c5abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('FacebookAI/xlm-roberta-base')\n",
    "batch_size = 32\n",
    "\n",
    "para_dataloader = DataLoader(para_dataset, batch_size= batch_size, shuffle=True, \n",
    "                            collate_fn=CollatorMASSIVEIntentClassSlotFill_para(tokenizer=tokenizer, max_length=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd037a66-600d-49d9-bbed-f61308fa085c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 39, 1, 14, 14]\n",
      "[1, 1, 1, 1, 1, 14, 14, 1, 39, 39, 1]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'slots_label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m para_dataloader:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/tl2546/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/scratch/tl2546/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/scratch/tl2546/.local/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[0;32m/scratch/tl2546/NLU_project/utils.py:177\u001b[0m, in \u001b[0;36mCollatorMASSIVEIntentClassSlotFill_para.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    173\u001b[0m padding_side \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpadding_side\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_side \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    175\u001b[0m     pad_tok_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslots_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28mlist\u001b[39m(label) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m] \u001b[38;5;241m*\u001b[39m (sequence_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(label)) \\\n\u001b[0;32m--> 177\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m pad_tok_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslots_label\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    178\u001b[0m     ]\n\u001b[1;32m    179\u001b[0m     pad_tok_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_slots\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28mlist\u001b[39m(target_label) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m] \u001b[38;5;241m*\u001b[39m (target_sequence_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(target_label)) \\\n\u001b[1;32m    181\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m pad_tok_inputs_target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_slots\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    182\u001b[0m     ]\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/tl2546/.local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:254\u001b[0m, in \u001b[0;36mBatchEncoding.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03mIf the key is a string, returns the value of the dict associated to `key` ('input_ids', 'attention_mask',\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03metc.).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03mwith the constraint of slice.\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings[item]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'slots_label'"
     ]
    }
   ],
   "source": [
    "for batch in para_dataloader:\n",
    "    print(batch)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8529a-b922-4362-8284-c7a10a7203c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
