{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27136ccd-ac13-4101-a6f4-47d20e435383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import Dataset, load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os \n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import joblib\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "violet = '#702b9d'\n",
    "RANDOM_SEED = 1002\n",
    "\n",
    "checkpoint = 'FacebookAI/xlm-roberta-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6890a86-35b0-4719-876e-43a962a1c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_train(example):\n",
    "    with open(os.getcwd() + '/data_en/en.intents', 'r', encoding = 'UTF-8') as file:\n",
    "        intent_labels_map = json.load(file)\n",
    "    \n",
    "    with open(os.getcwd() + '/data_en/en.slots', 'r', encoding = 'UTF-8') as file:\n",
    "        slot_labels_map = json.load(file)\n",
    "    \n",
    "        \n",
    "    label_to_pred_intent_idx = {v: k for k, v in intent_labels_map.items()}\n",
    "    \n",
    "    label_to_pred_slot_idx = {v: k for k, v in slot_labels_map.items()}\n",
    "    \n",
    "    converted_intent_labels = [int(label_to_pred_intent_idx.get(p, -100)) for p in example['target_intents']]  \n",
    "    converted_slot_labels = [[int(label_to_pred_slot_idx.get(s, -100)) for s in slot_seq] for slot_seq in example['target_slots']]\n",
    "    example['target_intents'], example['target_slots'] = converted_intent_labels, converted_slot_labels\n",
    "    return example\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db661412-97a7-44e8-ab3f-e6029b4dc7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train = Dataset.from_file(os.getcwd() + '/data_en/en.train/data-00000-of-00001.arrow')\n",
    "zh_train = Dataset.from_file(os.getcwd() + '/data_zh/zh.train/data-00000-of-00001.arrow')\n",
    "    \n",
    "zh_val = Dataset.from_file(os.getcwd() + '/data_zh/zh.dev/data-00000-of-00001.arrow')\n",
    "para_dataset = deepcopy(en_train)\n",
    "para_dataset = para_dataset.add_column(\"target_utt\", zh_train['utt'])\n",
    "para_dataset = para_dataset.add_column(\"target_slots\", zh_train['slots_str'])\n",
    "para_dataset = para_dataset.add_column(\"target_intents\", zh_train['intent_str'])\n",
    "para_dataset = para_dataset.map(lambda x: convert_train(x), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dec878e8-a8ad-4a4d-af7e-b744370c5abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('FacebookAI/xlm-roberta-base')\n",
    "batch_size = 32\n",
    "\n",
    "para_dataloader = DataLoader(para_dataset, batch_size= batch_size, shuffle=True, \n",
    "                            collate_fn=CollatorMASSIVEIntentClassSlotFill_para(tokenizer=tokenizer, max_length=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd037a66-600d-49d9-bbed-f61308fa085c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint found. Resuming training from epoch 1.\n",
      "  0%|                                                   | 0/255 [00:00<?, ?it/s]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|█████████████████████████████████████████| 255/255 [00:34<00:00,  7.42it/s]\n",
      "finish conversion to bio...\n",
      "Evaluate...\n",
      "Eval Loss: 5.4769\n",
      "Intent Accuracy: 0.06\n",
      "Slot F1: 0.18\n",
      "Exact Match Accuracy: 0.01\n"
     ]
    }
   ],
   "source": [
    "!python main_soft_align.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8529a-b922-4362-8284-c7a10a7203c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../MASSIVE/\n",
    "!python prepare_ds.py -d data/ -o data_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
