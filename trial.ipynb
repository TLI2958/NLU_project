{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27136ccd-ac13-4101-a6f4-47d20e435383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from datasets import Dataset, load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os \n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import joblib\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "violet = '#702b9d'\n",
    "RANDOM_SEED = 1002\n",
    "\n",
    "checkpoint = 'FacebookAI/xlm-roberta-base'\n",
    "batch_size = 12 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd037a66-600d-49d9-bbed-f61308fa085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaModel\n",
    "from main_soft_align_swap import *\n",
    "\n",
    "\n",
    "\n",
    "base_model = XLMRobertaModel.from_pretrained( checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained( checkpoint)\n",
    "Eval = namedtuple('Eval', ['predictions', 'label_ids'])\n",
    "\n",
    "en_train = Dataset.from_file(os.getcwd() + '/data_en/en.train/data-00000-of-00001.arrow')\n",
    "zh_train = Dataset.from_file(os.getcwd() + '/data_zh/zh.train/data-00000-of-00001.arrow')\n",
    "    \n",
    "zh_val = Dataset.from_file(os.getcwd() + '/data_zh/zh.dev/data-00000-of-00001.arrow')\n",
    "para_dataset = deepcopy(en_train)\n",
    "para_dataset = para_dataset.add_column(\"target_utt\", zh_train['utt'])\n",
    "para_dataset = para_dataset.add_column(\"target_slots\", zh_train['slots_str'])\n",
    "para_dataset = para_dataset.add_column(\"target_intents\", zh_train['intent_str'])\n",
    "para_dataset = para_dataset.map(lambda x: convert_train(x), batched=True)    \n",
    "\n",
    "para_dataloader = DataLoader(para_dataset, batch_size= batch_size, shuffle=True, \n",
    "                            collate_fn=CollatorMASSIVEIntentClassSlotFill_para(tokenizer=tokenizer, max_length=512))\n",
    "train_dataloader = DataLoader(en_train, batch_size= batch_size, shuffle=True, \n",
    "                            collate_fn=CollatorMASSIVEIntentClassSlotFill(tokenizer=tokenizer, max_length=512))\n",
    "eval_dataloader = DataLoader(zh_val, batch_size= batch_size, shuffle=True,\n",
    "                                collate_fn=CollatorMASSIVEIntentClassSlotFill(tokenizer=tokenizer, max_length=512))\n",
    "train_eval_dataloader = DataLoader(zh_train, batch_size= batch_size, shuffle=True,\n",
    "                                collate_fn=CollatorMASSIVEIntentClassSlotFill(tokenizer=tokenizer, max_length=512))\n",
    "vocab = tokenizer.get_vocab()\n",
    "vocab_size = len(vocab)\n",
    "\n",
    " # load mappings: should be using en mapping since order matters\n",
    "with open(os.getcwd() + '/data_en/en.intents', 'r', encoding = 'UTF-8') as file:\n",
    "    intent_labels_map = json.load(file)\n",
    "\n",
    "with open(os.getcwd() + '/data_en/en.slots', 'r', encoding = 'UTF-8') as file:\n",
    "    slot_labels_map = json.load(file)\n",
    "\n",
    "with open(os.getcwd() + '/data_zh/zh.intents', 'r', encoding = 'UTF-8') as file:\n",
    "    zh_intent_labels_map = json.load(file)\n",
    "\n",
    "with open(os.getcwd() + '/data_zh/zh.slots', 'r', encoding = 'UTF-8') as file:\n",
    "    zh_slot_labels_map =json.load(file)\n",
    "    \n",
    "\n",
    "base_model = XLMRobertaModel.from_pretrained('FacebookAI/xlm-roberta-base')\n",
    "model = MultiTaskICSL(base_model, vocab_size, num_slot_labels=56, num_intents=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2de8529a-b922-4362-8284-c7a10a7203c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278043648"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params  = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "base_params = sum(p.numel() for p in base_model.parameters() if p.requires_grad)\n",
    "base_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
